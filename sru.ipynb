{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "\n",
    "import nlp\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRULayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, highway_bias=-3.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.highway_bias = highway_bias\n",
    "        self.weight = torch.nn.Parameter(torch.empty(input_size, 3*input_size))\n",
    "        self.vector = torch.nn.Parameter(torch.empty(2*input_size))\n",
    "        self.bias = torch.nn.Parameter(torch.empty(2*input_size))\n",
    "        self.register_buffer(\"alpha\", torch.sqrt(torch.tensor(3.0)))\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def reset(self):\n",
    "        v = torch.sqrt(torch.tensor(3.0) / self.input_size)\n",
    "        torch.nn.init.uniform_(self.weight, a=-v, b=v)\n",
    "        torch.nn.init.uniform_(self.vector, a=-v, b=v)\n",
    "        self.bias.fill_(self.highway_bias)\n",
    "        \n",
    "    def forward(self, x, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor with size [N, T, F]\n",
    "        \"\"\"\n",
    "        batch = x.size(0)\n",
    "        if state is None:\n",
    "            cell = torch.zeros(batch, self.input_size, dtype=x.dtype, device=x.device)\n",
    "            hidden = torch.zeros(batch, self.input_size, dtype=x.dtype, device=x.device)\n",
    "        else:\n",
    "            cell = state[0]\n",
    "            hidden = state[1]\n",
    "            \n",
    "        u = x @ self.weight   # N, T, 3H\n",
    "        uw, uf, ur = u.split(self.input_size, dim=2)\n",
    "        \n",
    "        length = x.size(1)\n",
    "        out = torch.empty_like(x)\n",
    "        for i in range(length):\n",
    "            tf = (self.vector[:self.input_size] * cell) + self.bias[:self.input_size]\n",
    "            tr = (self.vector[self.input_size:] * cell) + self.bias[self.input_size:]\n",
    "\n",
    "            ft = torch.sigmoid(uf[:, i] + tf)\n",
    "            rt = torch.sigmoid(ur[:, i] + tr)\n",
    "\n",
    "            cell_t = ft * cell + (1 - ft) * uw[:, i]\n",
    "            hidden = rt * cell_t + (1 - rt) * x[:, i] * self.alpha\n",
    "            out[:, i] = hidden\n",
    "            cell = cell_t\n",
    "        \n",
    "        return out, (cell, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRU(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers, highway_bias=-3.0):\n",
    "        super().__init__()\n",
    "        self.srus = torch.nn.ModuleList([\n",
    "            #torch.jit.script(SRULayer(n_hidden, highway_bias))\n",
    "            SRULayer(n_hidden, highway_bias)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        h = x\n",
    "        new_h: List[torch.Tensor] = []\n",
    "        new_c: List[torch.Tensor] = []\n",
    "        for i, sru in enumerate(self.srus):\n",
    "            if state is not None:\n",
    "                s = state[0][i], state[1][i]\n",
    "            else:\n",
    "                s = None\n",
    "            h, new_s = sru(h, s)\n",
    "            new_h.append(new_s[0])\n",
    "            new_c.append(new_s[1])\n",
    "            \n",
    "        new_h_t = torch.stack(new_h)\n",
    "        new_c_t = torch.stack(new_c)\n",
    "        \n",
    "        return h, (new_h_t, new_c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 8\n",
    "# seq_len = 128\n",
    "# input_size = 64\n",
    "# n_layers = 2\n",
    "\n",
    "# x = torch.empty(batch, seq_len, input_size).normal_()\n",
    "\n",
    "# sru = torch.jit.script(SRU(input_size, n_layers)).cuda()\n",
    "# #sru = SRU(input_size, n_layers)\n",
    "# sru(x.cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sru(x.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, tokenizer, seq_len=32):\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.data = self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        all_data = []\n",
    "        for x in tqdm(nlp.load_dataset(\"wikitext\", split=self.split)):\n",
    "            if not x[\"text\"]:\n",
    "                continue\n",
    "            all_data.append(x[\"text\"].strip())\n",
    "            \n",
    "        enc_data = []\n",
    "        for d in tqdm(all_data):\n",
    "            enc_data.extend(self.tokenizer.encode(d).ids)\n",
    "        \n",
    "        return enc_data\n",
    "                   \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        base = idx * self.seq_len\n",
    "        x = self.data[base    :base     + self.seq_len]\n",
    "        y = self.data[base + 1:base + 1 + self.seq_len]\n",
    "\n",
    "        if len(x) < self.seq_len:\n",
    "            x = x + [0] * (self.seq_len - len(x))\n",
    "        if len(y) < self.seq_len:\n",
    "            y = y + [0] * (self.seq_len - len(y))\n",
    "            \n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(split='{self.split}', tokenizer={self.tokenizer})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        train_batch_size, \n",
    "        valid_batch_size=1, \n",
    "        vocab_size=20000, \n",
    "        n_layers=1,\n",
    "        n_hidden=512,\n",
    "        seq_len=256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.tokenizer = self._init_tokenizer()\n",
    "        self.train_dataset = WikiText(\"validation\", self.tokenizer, seq_len)\n",
    "        self.valid_dataset = WikiText(\"validation\", self.tokenizer, seq_len)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=n_hidden\n",
    "        )\n",
    "        \n",
    "        self.sru = torch.jit.script(SRU(n_hidden, n_layers))\n",
    "        \n",
    "        self.proj = torch.nn.Linear(\n",
    "            in_features=n_hidden,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "        \n",
    "        self.state = None\n",
    "        \n",
    "    def _init_tokenizer(self):\n",
    "        if not os.path.exists(\"data/vocab.json\"):\n",
    "            tokenizer = SentencePieceBPETokenizer()\n",
    "            tokenizer.train(\n",
    "                \"data/wikitext-103-raw/wiki.train.raw\",\n",
    "                vocab_size=self.vocab_size\n",
    "            )\n",
    "            tokenizer.save(\"data/\")\n",
    "        else:\n",
    "            tokenizer = SentencePieceBPETokenizer(\n",
    "                vocab_file=\"data/vocab.json\",\n",
    "                merges_file=\"data/merges.txt\"\n",
    "            )\n",
    "        return tokenizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.valid_dataset,\n",
    "            batch_size=self.valid_batch_size,\n",
    "            num_workers=8,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        h = self.embedding(x)\n",
    "        h, state = self.sru(h)\n",
    "        return self.proj(h), state\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat, state = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n",
    "        log = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": log}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            y_hat, state = self(x)\n",
    "            loss = F.cross_entropy(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n",
    "        return {\"val_loss\": loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = ((1.0 / len(outputs)) * sum([x['val_loss'].item() for x in outputs]))\n",
    "        try:\n",
    "            ppl = math.exp(loss)\n",
    "        except OverflowError:\n",
    "            ppl = float(\"inf\")\n",
    "        log = {\"val_loss\": loss, \"perplexity\": ppl}\n",
    "        return {'val_loss': ppl, \"log\": log}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [00:00<00:00, 104653.72it/s]\n",
      "100%|██████████| 2461/2461 [00:00<00:00, 10888.37it/s]\n",
      "100%|██████████| 3760/3760 [00:00<00:00, 98996.78it/s]\n",
      "100%|██████████| 2461/2461 [00:00<00:00, 10797.72it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    train_batch_size=128,\n",
    "    valid_batch_size=64,\n",
    "    vocab_size=8096,\n",
    "    seq_len=512,\n",
    "    n_layers=16,\n",
    "    n_hidden=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    precision=16,\n",
    "    val_check_interval=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name        | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0  | embedding   | Embedding             | 2 M   \n",
      "1  | sru         | RecursiveScriptModule | 3 M   \n",
      "2  | sru.srus    | RecursiveScriptModule | 3 M   \n",
      "3  | sru.srus.0  | RecursiveScriptModule | 197 K \n",
      "4  | sru.srus.1  | RecursiveScriptModule | 197 K \n",
      "5  | sru.srus.2  | RecursiveScriptModule | 197 K \n",
      "6  | sru.srus.3  | RecursiveScriptModule | 197 K \n",
      "7  | sru.srus.4  | RecursiveScriptModule | 197 K \n",
      "8  | sru.srus.5  | RecursiveScriptModule | 197 K \n",
      "9  | sru.srus.6  | RecursiveScriptModule | 197 K \n",
      "10 | sru.srus.7  | RecursiveScriptModule | 197 K \n",
      "11 | sru.srus.8  | RecursiveScriptModule | 197 K \n",
      "12 | sru.srus.9  | RecursiveScriptModule | 197 K \n",
      "13 | sru.srus.10 | RecursiveScriptModule | 197 K \n",
      "14 | sru.srus.11 | RecursiveScriptModule | 197 K \n",
      "15 | sru.srus.12 | RecursiveScriptModule | 197 K \n",
      "16 | sru.srus.13 | RecursiveScriptModule | 197 K \n",
      "17 | sru.srus.14 | RecursiveScriptModule | 197 K \n",
      "18 | sru.srus.15 | RecursiveScriptModule | 197 K \n",
      "19 | proj        | Linear                | 2 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5b989f4bc247049ba327f64f602e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
