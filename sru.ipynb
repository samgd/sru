{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "\n",
    "import nlp\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRUCell(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.weight = torch.nn.Parameter(torch.empty(input_size, 3*input_size))\n",
    "        self.vector = torch.nn.Parameter(torch.empty(2*input_size))\n",
    "        self.bias = torch.empty(2*input_size)\n",
    "        self.register_buffer(\"alpha\", torch.sqrt(torch.tensor(3.0)))\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        v = torch.sqrt(torch.tensor(3.0) / self.input_size)\n",
    "        torch.nn.init.uniform_(self.weight, a=-v, b=v)\n",
    "        torch.nn.init.uniform_(self.vector, a=-v, b=v)\n",
    "        self.bias.zero_()\n",
    "        \n",
    "    def forward(self, x, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor with size [N, F]\n",
    "        \"\"\"\n",
    "        if state is None:\n",
    "            batch = x.size(0)\n",
    "            cell = torch.zeros(batch, self.input_size)\n",
    "            hidden = torch.zeros(batch, self.input_size)\n",
    "        else:\n",
    "            cell = state[0]\n",
    "            hidden = state[1]\n",
    "            \n",
    "        u = x @ self.weight   # N, 3H\n",
    "        uw, uf, ur = u.split(self.input_size, dim=1)\n",
    "        \n",
    "        v = (self.vector.view(2, self.input_size) * cell).view(2*self.input_size) + self.bias\n",
    "        tf, tr = v.split(self.input_size)\n",
    "        \n",
    "        ft = torch.sigmoid(uf + tf)\n",
    "        rt = torch.sigmoid(ur + tr)\n",
    "        \n",
    "        cell_t = ft * cell + (1 - ft) * uw\n",
    "        hidden_t = rt * cell_t + (1 - rt) * x * self.alpha\n",
    "        \n",
    "        return hidden_t, (cell_t, hidden_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRU(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.cell = torch.jit.script(SRUCell(input_size))\n",
    "        \n",
    "    def forward(self, x, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor with size [N, T, F]\n",
    "        \"\"\"\n",
    "        length = x.size(1)\n",
    "        state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "        out = torch.empty_like(x)\n",
    "        for i in range(length):\n",
    "            out[:, i], state = self.cell(x[:, i])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "seq_len = 100\n",
    "input_size = 64\n",
    "\n",
    "x = torch.empty(batch, seq_len, input_size).normal_()\n",
    "\n",
    "sru = torch.jit.script(SRU(input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0393, -1.4393, -1.8755,  ...,  1.8419,  0.0745, -1.4392],\n",
       "         [ 1.8173, -0.1810, -1.0920,  ...,  0.0106, -0.7744, -1.5187],\n",
       "         [-0.6281,  0.2251,  0.8739,  ...,  0.3141, -2.8450,  0.2109],\n",
       "         ...,\n",
       "         [ 1.8597,  1.2647, -1.4092,  ...,  0.3748,  0.1780,  0.8282],\n",
       "         [-0.0764,  1.6122,  0.6091,  ...,  0.6725, -0.1031, -0.0130],\n",
       "         [ 0.0983,  0.8663, -0.5963,  ...,  0.8382, -1.1926,  0.6968]],\n",
       "\n",
       "        [[ 0.5164,  0.2064, -0.9087,  ...,  0.8550,  1.5220, -1.8969],\n",
       "         [ 0.1648, -1.2897, -0.5557,  ...,  1.0085,  0.1047, -1.3821],\n",
       "         [ 0.1513,  0.5151, -0.3657,  ...,  0.1641, -0.2462,  0.5926],\n",
       "         ...,\n",
       "         [ 0.2457,  2.7698,  0.9129,  ..., -0.6625, -1.0116, -0.3728],\n",
       "         [ 0.7080, -1.7702,  0.5850,  ...,  1.1599, -0.7030, -1.2377],\n",
       "         [-2.2299, -1.6632, -0.5999,  ..., -1.7677, -0.3579, -1.3820]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sru(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n, drop_short=True):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        c = lst[i:i + n]\n",
    "        if len(c) == n:\n",
    "            yield c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, tokenizer, batch_size=1, seq_len=32):\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.data = self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        all_data = []\n",
    "        tot_len = 0\n",
    "        for x in nlp.load_dataset(\"wikitext\", split=self.split):\n",
    "            if not x[\"text\"]:\n",
    "                continue\n",
    "            all_data.append(x[\"text\"].strip())\n",
    "            tot_len += len(x[\"text\"])\n",
    "            \n",
    "        data = []\n",
    "        max_chunk = math.ceil(tot_len / self.batch_size)\n",
    "        cur_chunk = 0\n",
    "        cur_data = []\n",
    "        for sentence in all_data:\n",
    "            cur_data.append(sentence)\n",
    "            cur_chunk += len(sentence)\n",
    "            if cur_chunk >= max_chunk:\n",
    "                c_ids = []\n",
    "                for c in chunks(self.tokenizer.encode(\" \".join(cur_data)).ids, self.seq_len):\n",
    "                    c_ids.append(c)\n",
    "                data.append(c_ids)\n",
    "                cur_chunk = 0\n",
    "                cur_data = []\n",
    "                \n",
    "        if cur_data:\n",
    "            c_ids = []\n",
    "            for c in chunks(self.tokenizer.encode(\" \".join(cur_data)).ids, self.seq_len):\n",
    "                c_ids.append(c)\n",
    "            data.append(c_ids)\n",
    "\n",
    "            max_len = max([len(chunk) for chunk in data])\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i][:max_len]\n",
    "                \n",
    "        return data\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor([chunk[idx] for chunk in self.data], dtype=torch.long)\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(split='{self.split}', tokenizer={self.tokenizer})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, train_batch_size, valid_batch_size=1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = self._init_tokenizer()\n",
    "        self.train_dataset = WikiText(\"train\", self.tokenizer, train_batch_size)\n",
    "        self.valid_dataset = WikiText(\"validation\", self.tokenizer, valid_batch_size)\n",
    "        \n",
    "    def _init_tokenizer(self):\n",
    "        if not os.path.exists(\"data/vocab.json\"):\n",
    "            tokenizer = SentencePieceBPETokenizer()\n",
    "            tokenizer.train(\n",
    "                \"data/wikitext-103-raw/wiki.train.raw\",\n",
    "                vocab_size=20000\n",
    "            )\n",
    "            tokenizer.save(\"data/\")\n",
    "        else:\n",
    "            tokenizer = SentencePieceBPETokenizer(\n",
    "                vocab_file=\"data/vocab.json\",\n",
    "                merges_file=\"data/merges.txt\"\n",
    "            )\n",
    "        return tokenizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=None,\n",
    "        )\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.valid_dataset,\n",
    "            batch_size=None,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    train_batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045, 12170, 13261, 13780,  3671,  1090,  1045, 12170, 13261, 13780,\n",
       "          3671,  1090,  1008,  1775,  1089,  1005,  3294,  8619,  2932,  1185,\n",
       "          2587,  8619,  2932,  1008,  1121,  1003,  2244,  1026,  1266,  1436,\n",
       "          1011,  8619],\n",
       "        [ 1045,  1045,  3379,  1030,  2420,  1045,  1045,  9118,  1028, 12997,\n",
       "          1008, 10087,  1008,  1120,  3172,    72,  1687,  1962,  3722, 14273,\n",
       "          1089,  1003,  2154,  1008,  1028, 18541,  1008,  4096,  1008,  1033,\n",
       "          6337,  1016]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(model.valid_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
